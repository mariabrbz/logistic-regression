{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Neural Network: Implementation and Image Classification\n",
    "## Cat or not a cat: part II\n",
    "\n",
    "The purpose of this notebook is to apply a neural network with L layers to the data used in the `logistic_regression` notebook, trying to improve the accuracy of its classification. It also uses notation previously explained in the `shallow_neural_network` notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (5.0, 4.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part I: building a deep neural network\n",
    "\n",
    "We will implement auxiliary functions that will ease the implementation of the model functions. In Part II, we will use these functions to build a 2 layer and an L layer neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Activation functions\n",
    "\n",
    "We will use a ReLU function for the first $L-1$ layers and a sigmoid function for the last one (since we need a result between $0$ and $1$). If $g$ is the activation function, \n",
    "`sigmoid_backward` and `relu_backward` compute $$dZ^{[l]} = dA^{[l]} * g'(Z^{[l]}) \\tag{11}$$.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    sigm = 1 / (1 + np.exp(-Z))\n",
    "    return (sigm, Z)\n",
    "\n",
    "def relu(Z):\n",
    "    rel = np.maximum(0, Z)\n",
    "    return (rel, Z)\n",
    "\n",
    "def sigmoid_backward(dA, cache):\n",
    "    Z = cache \n",
    "    sigm = 1 / (1 + np.exp(-Z))\n",
    "    dZ = dA * sigm * (1 - sigm)\n",
    "    return dZ\n",
    "\n",
    "def relu_backward(dA, cache):\n",
    "    Z = cache\n",
    "    dZ = np.array(dA, copy = True)\n",
    "    dZ[Z <= 0] = 0\n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Initializing the parameters\n",
    "\n",
    "We initiate the weights randomly and the biases with $0$. For the deep neural network, we use the array layer_dims, in which we store the number of units in each layer of the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#2 layer NN\n",
    "def initialize_parameters(n_x, n_h, n_y):\n",
    "    np.random.seed(1)\n",
    "    \n",
    "    W1 = np.random.randn(n_h, n_x) * 0.01\n",
    "    b1 = np.zeros((n_h, 1))\n",
    "    W2 = np.random.randn(n_y, n_h) * 0.01\n",
    "    b2 = np.zeros((n_y, 1))\n",
    "    \n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "    \n",
    "    return parameters    \n",
    "\n",
    "#L layer NN\n",
    "def initialize_parameters_deep(layer_dims):\n",
    "    np.random.seed(3)\n",
    "    parameters = {}\n",
    "    L = len(layer_dims)            #number of layers in the network\n",
    "\n",
    "    for l in range(1, L):\n",
    "        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l - 1]) * 0.01\n",
    "        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3 Forward propagation\n",
    "##### a. Linear forward \n",
    "$$Z^{[l]} = W^{[l]}A^{[l-1]} +b^{[l]}, \\qquad A^{[0]} = X$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linear_forward(A, W, b):\n",
    "    Z = np.dot(W, A) + b\n",
    "    cache = (A, W, b)    #store cache for the backward propagation\n",
    "    \n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### b. Linear activation forward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$A^{[l]} = g(Z^{[l]}) = g(W^{[l]}A^{[l-1]} +b^{[l]})$ where the activation $g$ can be sigmoid() or relu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "    Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "    \n",
    "    if activation == \"sigmoid\":\n",
    "        A, activation_cache = sigmoid(Z)\n",
    "    \n",
    "    elif activation == \"relu\":\n",
    "        A, activation_cache = relu(Z)\n",
    "\n",
    "    cache = (linear_cache, activation_cache)\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### c. Linear activation forward for deep NN\n",
    "\n",
    "When implementing the $L$ layer Neural Network, we will use ReLU $L-1$ times and sigmoid once.\n",
    "\n",
    "$A^{[L]} = \\sigma(Z^{[L]}) = \\sigma(W^{[L]} A^{[L-1]} + b^{[L]})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def L_model_forward(X, parameters):\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(parameters) // 2                  #number of layers in the neural network\n",
    "    \n",
    "    for l in range(1, L):\n",
    "        A_prev = A \n",
    "        A, cache = linear_activation_forward(A_prev, parameters[\"W\" + str(l)], parameters[\"b\" + str(l)], \"relu\")\n",
    "        caches.append(cache)\n",
    "    \n",
    "    AL, cache = linear_activation_forward(A, parameters[\"W\" + str(L)], parameters[\"b\" + str(L)], \"sigmoid\")\n",
    "    caches.append(cache)\n",
    "    \n",
    "    assert(AL.shape == (1,X.shape[1]))\n",
    "    return AL, caches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Cost function\n",
    "\n",
    "The cost function has the following formula: $$-\\frac{1}{m} \\sum\\limits_{i = 1}^{m} (y^{(i)}\\log\\left(a^{[L] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[L](i)}\\right))$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_cost(AL, Y):\n",
    "    m = Y.shape[1]    #number of examples\n",
    "    cost = - (1 / m) * np.sum(np.dot(Y, np.log(AL).T) + np.dot(1 - Y, np.log(1 - AL).T))\n",
    "    cost = np.squeeze(cost)\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Backward propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### a. Linear backward\n",
    "\n",
    "$$ dW^{[l]} = \\frac{\\partial \\mathcal{J} }{\\partial W^{[l]}} = \\frac{1}{m} dZ^{[l]} A^{[l-1] T}$$\n",
    "$$ db^{[l]} = \\frac{\\partial \\mathcal{J} }{\\partial b^{[l]}} = \\frac{1}{m} \\sum_{i = 1}^{m} dZ^{[l](i)}$$\n",
    "$$ dA^{[l-1]} = \\frac{\\partial \\mathcal{L} }{\\partial A^{[l-1]}} = W^{[l] T} dZ^{[l]}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linear_backward(dZ, cache):\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "\n",
    "    dW = (1 / m) * np.dot(dZ, A_prev.T)\n",
    "    db = (1 / m) * np.sum(dZ, axis=1, keepdims=True)\n",
    "    dA_prev = np.dot(W.T, dZ)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### b. Linear activation backward "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linear_activation_backward(dA, cache, activation):\n",
    "    linear_cache, activation_cache = cache\n",
    "    \n",
    "    if activation == \"relu\":\n",
    "        dZ = relu_backward(dA, activation_cache)\n",
    "        \n",
    "    elif activation == \"sigmoid\":\n",
    "        dZ = sigmoid_backward(dA, activation_cache)\n",
    "        \n",
    "    dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### c. Linear activation backward for deep NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def L_model_backward(AL, Y, caches):\n",
    "    grads = {}\n",
    "    L = len(caches)         #number of layers\n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape)\n",
    "    \n",
    "    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
    "    \n",
    "    current_cache = caches[-1]\n",
    "    grads[\"dA\" + str(L-1)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_activation_backward(dAL, current_cache, \"sigmoid\")\n",
    "    \n",
    "    for l in reversed(range(L-1)):\n",
    "        current_cache = caches[l]\n",
    "        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[\"dA\" + str(l + 1)], current_cache, \"relu\")\n",
    "        grads[\"dA\" + str(l)] = dA_prev_temp\n",
    "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
    "        grads[\"db\" + str(l + 1)] = db_temp\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Updating parameters\n",
    "\n",
    "$$ W^{[l]} = W^{[l]} - \\alpha \\text{ } dW^{[l]}$$\n",
    "$$ b^{[l]} = b^{[l]} - \\alpha \\text{ } db^{[l]} $$\n",
    "\n",
    "where $\\alpha$ is the learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    L = len(parameters) // 2     #number of layers\n",
    "\n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate * grads[\"dW\" + str(l+1)]\n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate * grads[\"db\" + str(l+1)]\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part II: Image Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading and modifying the data\n",
    "def load_dataset():\n",
    "    train_dataset = h5py.File('train_catvnoncat.h5', \"r\")\n",
    "    initial_train_set_x = np.array(train_dataset[\"train_set_x\"][:])  \n",
    "    train_set_y = np.array(train_dataset[\"train_set_y\"][:])\n",
    "\n",
    "    test_dataset = h5py.File('test_catvnoncat.h5', \"r\")\n",
    "    initial_test_set_x = np.array(test_dataset[\"test_set_x\"][:])       \n",
    "    test_set_y = np.array(test_dataset[\"test_set_y\"][:])\n",
    "\n",
    "    train_set_x = initial_train_set_x.reshape(initial_train_set_x.shape[0], -1).T / 255\n",
    "    test_set_x = initial_test_set_x.reshape(initial_test_set_x.shape[0], -1).T / 255\n",
    "    train_set_y = train_set_y.reshape((1, train_set_y.shape[0])) \n",
    "    test_set_y = test_set_y.reshape((1, test_set_y.shape[0])) \n",
    "    \n",
    "    return initial_test_set_x, initial_train_set_x, train_set_x, train_set_y, test_set_x, test_set_y\n",
    "\n",
    "initial_test_set_x, initial_train_set_x, train_set_x, train_set_y, test_set_x, test_set_y = load_dataset()\n",
    "\n",
    "#function that transforms label (1/0) into cat/not a cat\n",
    "def binary_to_cat(x):\n",
    "    if x:\n",
    "        return \"a cat\"\n",
    "    return \"not a cat\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Two-layer neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define constants\n",
    "n_x = 12288     #X.shape[0]\n",
    "n_h = 7         #units in hidden layer\n",
    "n_y = 1\n",
    "layers_dims = (n_x, n_h, n_y)\n",
    "\n",
    "def two_layer_model(X, Y, layers_dims, learning_rate = 0.0075, num_iterations = 3000, show_cost=False):\n",
    "    np.random.seed(1)\n",
    "    grads = {}\n",
    "    costs = []                              \n",
    "    m = X.shape[1]\n",
    "    (n_x, n_h, n_y) = layers_dims\n",
    "\n",
    "    parameters = initialize_parameters(n_x, n_h, n_y)\n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    \n",
    "    for i in range(0, num_iterations):\n",
    "        A1, cache1 = linear_activation_forward(X, W1, b1, \"relu\")\n",
    "        A2, cache2 = linear_activation_forward(A1, W2, b2, \"sigmoid\")\n",
    "        \n",
    "        cost = compute_cost(A2, Y)\n",
    "\n",
    "        dA2 = - (np.divide(Y, A2) - np.divide(1 - Y, 1 - A2))\n",
    "        dA1, dW2, db2 = linear_activation_backward(dA2, cache2, \"sigmoid\")\n",
    "        dA0, dW1, db1 = linear_activation_backward(dA1, cache1, \"relu\")\n",
    "\n",
    "        grads['dW1'] = dW1\n",
    "        grads['db1'] = db1\n",
    "        grads['dW2'] = dW2\n",
    "        grads['db2'] = db2\n",
    "        \n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "        W1 = parameters[\"W1\"]\n",
    "        b1 = parameters[\"b1\"]\n",
    "        W2 = parameters[\"W2\"]\n",
    "        b2 = parameters[\"b2\"]\n",
    "        \n",
    "        if show_cost and i % 100 == 0:\n",
    "            print(\"Cost after iteration {}: {}\".format(i, np.squeeze(cost)))\n",
    "            costs.append(cost)\n",
    "       \n",
    "    #plot the cost\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per hundreds)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.6930497356599888\n",
      "Cost after iteration 100: 0.6464320953428849\n",
      "Cost after iteration 200: 0.6325140647912677\n",
      "Cost after iteration 300: 0.6015024920354665\n",
      "Cost after iteration 400: 0.5601966311605747\n",
      "Cost after iteration 500: 0.5158304772764729\n",
      "Cost after iteration 600: 0.47549013139433255\n",
      "Cost after iteration 700: 0.43391631512257495\n",
      "Cost after iteration 800: 0.4007977536203886\n",
      "Cost after iteration 900: 0.3580705011323798\n",
      "Cost after iteration 1000: 0.3394281538366412\n",
      "Cost after iteration 1100: 0.3052753636196264\n",
      "Cost after iteration 1200: 0.2749137728213015\n",
      "Cost after iteration 1300: 0.24681768210614846\n",
      "Cost after iteration 1400: 0.19850735037466108\n",
      "Cost after iteration 1500: 0.17448318112556654\n",
      "Cost after iteration 1600: 0.17080762978096023\n",
      "Cost after iteration 1700: 0.11306524562164728\n",
      "Cost after iteration 1800: 0.09629426845937154\n",
      "Cost after iteration 1900: 0.08342617959726861\n",
      "Cost after iteration 2000: 0.07439078704319084\n",
      "Cost after iteration 2100: 0.06630748132267932\n",
      "Cost after iteration 2200: 0.05919329501038171\n",
      "Cost after iteration 2300: 0.053361403485605564\n",
      "Cost after iteration 2400: 0.04855478562877018\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAU4AAAEWCAYAAAAJjn7zAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n5MHpnHO19P8Babwke+3Nvm8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 360x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "parameters = two_layer_model(train_set_x, train_set_y, layers_dims = (n_x, n_h, n_y), num_iterations = 2500, show_cost = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 100.0 %\n",
      "test accuracy: 72.0 %\n"
     ]
    }
   ],
   "source": [
    "def predict(X, parameters):\n",
    "    A2, _ = L_model_forward(X, parameters)\n",
    "    predictions = (A2 > 0.5)\n",
    "    return predictions\n",
    "\n",
    "print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(predict(train_set_x, parameters) - train_set_y)) * 100))\n",
    "print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(predict(test_set_x, parameters) - test_set_y)) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the test accuracy has improved by 2% compared to the logistic regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. L-layer neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers_dims = [12288, 20, 7, 5, 1]\n",
    "\n",
    "def L_layer_model(X, Y, layers_dims, learning_rate = 0.0075, num_iterations = 3000, show_cost=False):\n",
    "    np.random.seed(1)\n",
    "    costs = []                      \n",
    "    parameters = initialize_parameters_deep(layers_dims)\n",
    "\n",
    "    for i in range(0, num_iterations):\n",
    "        AL, caches = L_model_forward(X, parameters)\n",
    "        cost = compute_cost(AL, Y)\n",
    "        grads = L_model_backward(AL, Y, caches)\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "\n",
    "        if show_cost and i % 100 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" % (i, cost))\n",
    "            costs.append(cost)\n",
    "            \n",
    "    #plot cost\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per hundreds)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = L_layer_model(train_set_x, train_set_y, layers_dims, num_iterations = 2500, show_cost = True)\n",
    "\n",
    "print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(predict(train_set_x, parameters) - train_set_y)) * 100))\n",
    "print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(predict(test_set_x, parameters) - test_set_y)) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this model, we get an accuracy of 80% on a training set, which is significantly hire than both the logistic regression and the 2-layer model."
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "neural-networks-deep-learning",
   "graded_item_id": "c4HO0",
   "launcher_item_id": "lSYZM"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
